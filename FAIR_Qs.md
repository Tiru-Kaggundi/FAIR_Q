**[Framework for Artificial Intelligence Regulation -- Questionnaire
(FAIR-Q)]{.underline}**

**Introduction:**

Artificial Intelligence (AI) tools used in governance require a great
degree of scrutiny before they are launched. This is due to the
potential of AI tools to create a disproportionate impact when used to
achieve a public policy goal. To mitigate the risks, a Framework for AI
Regulation -- Questionnaire (FAIR-Q) is proposed, which can be used by
regulators or government departments implementing these AI based
solutions.

**Recommendation:**

FAIR-Q should be adopted by any department that proposes use of any AI
tool for governance. FAIR-Q covers questions related to the need for AI,
algorithms, processes, fairness, accountability, and ethical issues. The
government department and the AI team should be able to satisfactorily
answer these before the tool is allowed into public service. The
questions are designed to remain simple, yet effective.

**Relevance:**

AI tools can embed and exacerbate biases and inequalities found in the
data that is used to train the tool. This raises important questions
about fairness, accountability, and ethics. FAIR-Q ensures that these
questions are adequately addressed.

FAIR-Q is an interim tool to mitigate adverse effects arising from use
of AI tools in governance while formal standards and guidelines evolve
on the subject.

**\
**

**What is AI in governance?**

When there are clear rules for a process, they can be coded into
software, and computers can do the work faster and at a much larger
scale than manual efforts. This is also true for governance and public
service processes. Examples include bank transactions, applications for
government services, and direct subsidy transfers. This type of
computerization in governance began in the 1990s and continues till
date. Various public service delivery processes are computerized using
software that codify the rules.

AI is the next level and touted as the futuristic solution to solving
most problems at scale. The computers learn the rules on their own,
based on the examples shown to it, to make an AI tool.

We call such examples training data. The learning process is called
'training' or

'machine-learning'. The code that helps the computer to learn is called
a 'training algorithm'. Once trained, the tool is shown new test
examples which the AI has never seen. The AI is evaluated based on how
well it performs on these unseen test examples. The trained tool, ready
for deployment, is the 'AI'. As the computer learns the rules on its
own, we call it intelligent. For example, to create an AI tool that
detects tax evaders, the training algorithm is shown thousands of
examples of tax evasion, and thousands more where there was no tax
evasion. The training algorithm picks up the patterns from the examples
to understand features that contribute to tax evasion. Given enough
examples, the accuracy improves, and we get an AI tool. The cases that
are shown to the training algorithm are labelled examples, that is,
someone identifies these cases as a tax evasion case or otherwise. This
type of training is called supervised learning. Most AI applications in
governance are of the supervised type.

AI tools are powerful. Their performance, on the narrow tasks that they
are designed to perform, is better than humans. We have AI Chess and Go
grandmasters that can beat the best human players. The AI X-ray
pathology detectors perform better than human pathologists today.
Similarly, an AI tool is expected to perform well on governance
tasks.Artificial Intelligence tools are being increasingly deployed
across the world. In the hands of the government, they are powerful
tools that can be leveraged for public welfare.

**Algorithms can go wrong**

AI algorithms are black boxes for most people, except for the
engineering teams that work on them, or the AI researchers. The inner
workings of some of the advanced AI algorithms are not well understood.
The engineering teams that develop these tools also struggle to explain
the working of these algorithms. It's difficult to understand what
'rules' the AI has learnt when a complex learning algorithm is used. An
AI model with great performance doesn\'t translate to understanding of
the tool\'s inner workings.

Many AI researchers are working to make AI systems explainable, to
themselves first, and to the larger world subsequently. When Google's
team initially developed an AI based medical pathology detector to
detect pathologies based on X-ray images, the AI showed great
performance on the test data. However, the performance of the AI on
fresh images was bad. When the team looked closely, they found that the
AI was looking at the pen marks left by the pathologists on the training
images. Rather than learning pathology, the tool learned that more pen
marks indicate the presence of pathology (Mullainathan 2021). As these
pen marks were absent in fresh images, the AI tool failed. This has
since been rectified. It is likely that if the team had not looked close
enough, the tool might have been released into the wild based on the
initial test performance. We have examples where algorithms have gone
unintentionally wrong.

For a long time, image search for 'CEO' on google images threw up only
white men in suits. Racial biases found in AI tools such as COMPAS that
was used to detect recidivism in undertrial prisoners in the US is
another such case. (Julia Angwin 2016)

**Processes and Standards for AI -- Current state**

The legal guardrails to prevent unintended consequences arising from AI
tools in the public domain are being deliberated by the governments and
multilateral institutions. OECD's efforts to codify [[AI
principles](https://oecd.ai/en/ai-principles)]{.underline} (OECD 2019)
and UNESCO's recommendations on [[ethics of
AI](https://en.unesco.org/news/unesco-member-states-adopt-first-ever-global-agreement-ethics-artificial-intelligence)]{.underline}
(UNESCO

2021\) are initial efforts to ringfence the bad effects of AI in the
hands of a strong government.

The EU has proposed an [[AI
Act](https://eur-lex.europa.eu/resource.html?uri=cellar:e0649735-a372-11eb-9585-01aa75ed71a1.0001.02/DOC_1&format=PDF)]{.underline}
(European Commission 2022) which is being deliberated in Brussels and
which may act as a reference for other governments. However, [[it is
challenging to]{.underline} [create sound legal and regulatory framework
for a new and evolving subject like
AI]{.underline}](https://themarkup.org/news/2022/01/04/why-its-so-hard-to-regulate-algorithms)
(Feathers 2022). At the current state of development in the field, we
are in a place where availability, ease of deployment of such tools, and
their impact on the society outpaces the development of regulatory
framework.

**The interim solution**

Good design and correct processes can prevent AI tools from committing
serious errors. While the regulatory framework and standards evolve,
there is a need for interim practical tools and methods on thinking
about AI. These can be used by the department heads, parliamentary
committees, public policymakers, and the judiciary. Asking meaningful
questions at the right time can help steer AI based tools in the right
direction and ensure that they do no harm to the public. By asking these
questions one can be reasonably assured about the correctness of inputs
and the learning process.

Framework for AI Regulation -- Questionnaire (FAIR-Q) proposes a set of
three questionnaires. These questions are adapted from various sources.
They are based on the experiences across the world, managerial ways of
thinking about AI from top business schools, and best practices gleaned
from documents released by various agencies.

The questions fall into three broad categories:

a)  Business case and Governance process redesign for AI

b)  Algorithm design

c)  Fairness, accountability, and ethics

Each question seeks a specific type of non-technical response from the
department that owns the AI tool and the associated engineering team.
The answer would help policy makers gauge the impact and understand any
potential side effect of the AI tool on the public. Each question also
provides a brief note on the purpose of the question and the blunder it
tries to prevent.

**Business case and Governance Process questions**

AI for the sake of AI is not a good approach especially when simpler
rule based programming could work. A clear demonstrable competitive
advantage should arise out of usage of AI. There should be clear
justification for developing and using an AI tool in governance.

The approved process flow in many government departments does not permit
automated AI tool-based decisions. Using AI may lead to legal challenges
especially if an AI is used for initiating investigative actions. The
grounds for legal challenge could be violation of principles of natural
justice, or executive overreach.

Business and process design questions are designed to reduce the risk by
overextending AI without justification. They also prevent the use of AI
at wrong places in public governance processes. **(FAIR-Q : Appendix
A)**

# **Algorithm design questions**

Algorithmic design questions ensure that the AI tool is designed to the
specification of the policy makers. These questions attempt to evoke
answers that translate machine learning jargons into plain speak. They
ensure that the AI tool is trained on correct data that matches the
deployment scenario. The evaluation metrics are also covered through
these questions. AI tools should be evaluated on customized evaluation
criteria keeping in mind their intended application in the public domain
and should not blindly rely on the usual accuracy and precision measures
used by the machine learning community. **(FAIR-Q : Appendix B)**

**Fairness, accountability, and ethics of AI questions**

AI in governance should be fair, unbiased, and ethical. There should be
clear accountability established for usage of AI for governance
purposes. Departments should not be allowed to hold the AI tool
accountable for any wrong decision. At the least, the AI tools should
not violate the fundamental rights and ethical principles as outlined by
UNESCO or OECD. These questions probe ethical aspects of AI. Each
question is linked to a specific recommendation on fairness, bias, and
ethics. **(FAIR-Q: Appendix C)**

**Conclusion:**

FAIR-Q is designed to remain simple, yet effective enough to serve the
purpose of guiding the development of the AI tool. FAIR-Q asks
penetrating questions to prevent harm and seek answers in clear language
that could be understood by policy makers and administrators. They
ensure that the AI team introspects deeply, and clearly explains as to
how the AI tool furthers the cause of good governance in the country.

# Bibliography

European Commission. 2022. *eur-lex.europa.eu.* May. Accessed May 19,
2022.
https://eur-lex.europa.eu/resource.html?uri=cellar:e0649735-a372-11eb-9585-01aa75ed71a1.0001.02/DOC_1&format=PDF.

Feathers, Todd. 2022. *The Markup.* Jan 4. Accessed May 19, 2022.
https://themarkup.org/news/2022/01/04/why-its-so-hard-to-regulate-algorithms.

Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner, ProPublica.
2016. *ProPublica.* May 23. Accessed May 19, 2022.
https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing.

Mullainathan, Sendhil. 2021. *Gold lab foundation.* Accessed May 19,
2022.
https://goldlabfoundation.org/wp-content/uploads/2021/05/GLS-2021_Presentation_Mullainathan.pdf.

OECD. 2019. *OECD.AI.* May. Accessed May 2022.
https://oecd.ai/en/ai-principles.

UNESCO. 2021. *unesco.org.* Nov 25. Accessed May 19, 2022.
https://en.unesco.org/news/unesco-member-states-adopt-first-ever-global-agreement-ethics-artificial-intelligence.

**Appendix A -- Business case and Process design related questions**

+--------+----------+---------------------------+---------------------+
| Ca     | Que      | Expected answer           | Purpose             |
| tegory | stion(s) |                           |                     |
+========+==========+===========================+=====================+
| Bu     | What is  | The department/team       | a.  Prevents        |
| siness | the      | should demonstrate the    |     irrational      |
| case   | justi    | advantage of usage of AI  |     exuberance      |
|        | fication | for the specific case     |     about AI        |
|        | for use  | (general advantages of AI |     capabilities    |
|        | of AI,   | is not an acceptable      |     leading to      |
|        | over the | answer). The answer       |     unnecessary     |
|        | normal   | should explain why normal |     usage of AI     |
|        | rule     | rule based approach would |     where such      |
|        | based    | fail and why an AI system |     tools are not   |
|        | pro      | would be able to perform  |     warranted       |
|        | gramming | better. Any benchmark     |                     |
|        | in this  | performance improvements  | b.  Ensures that    |
|        | case?    | expected should also be   |     minimum         |
|        |          | shown, if available.      |     benchmark       |
|        | What     |                           |     performance     |
|        | per      | **Example answer (for AI  |     expectations    |
|        | formance | based localized weather   |     are set before  |
|        | enha     | forecast):** It is        |     significantly   |
|        | ncements | difficult to codify       |     investing into  |
|        | in       | weather predictions using |     development     |
|        | public   | normal programming        |                     |
|        | service  | methods. The AI system    | c.  Clear           |
|        | delivery | has consistently          |     advantages of   |
|        | arises   | outperformed rule based   |     AI tool are     |
|        | due to   | local weather             |     established for |
|        | the      | predictions. Therefore,   |     public policy   |
|        | proposed | AI is being suggested for |     purposes        |
|        | AI tool? | use here to help farmers  |                     |
|        |          | at local level by         |                     |
|        |          | providing accurate,       |                     |
|        |          | localized weather         |                     |
|        |          | predictions. Evaluation   |                     |
|        |          | measures confirm better   |                     |
|        |          | performance.              |                     |
+--------+----------+---------------------------+---------------------+
| P      | At what  | The answer should show    | a.  Helps gauge the |
| rocess | stage    | the step where AI tool    |     centrality and  |
| design | does the | comes in the department's |     role of AI tool |
|        | AI come  | process flow chart. If    |     in the whole    |
|        | into the | the AI based decisions    |     process         |
|        | process  | are used for any          |                     |
|        | flow?    | investigative actions,    | b.  Prevents        |
|        |          | necessary legal           |     overstepping    |
|        | Show the | support/legislative       |     legislative and |
|        | process  | approval/business rules   |     administrative  |
|        | diagram  | for such actions should   |     mandate in      |
|        | with AI  | be clearly explained with |     using the AI    |
|        | and      | list of all approvals     |     tool            |
|        | dem      | needed or taken.          |                     |
|        | onstrate |                           |                     |
|        | the      | **Example answer (for AI  |                     |
|        | legal or | tool that detects tax     |                     |
|        | business | evaders):** The AI tool   |                     |
|        | rules    | comes at the time of tax  |                     |
|        | that     | scrutiny. The AI flags    |                     |
|        | allow    | suspicious cases for      |                     |
|        | the use  | human tax inspector for   |                     |
|        | of such  | detailed examination. As  |                     |
|        | tool.    | the tool doesn't initiate |                     |
|        |          | investigative actions on  |                     |
|        |          | its own and relies on     |                     |
|        |          | human tax inspector for   |                     |
|        |          | further verifications and |                     |
|        |          | for initiating any        |                     |
|        |          | investigations under the  |                     |
|        |          | Income Tax Act, no        |                     |
|        |          | separate approval is      |                     |
|        |          | required.                 |                     |
+--------+----------+---------------------------+---------------------+
| P      | Was any  | Some AI tools may require | a.  Prevents        |
| rocess | existing | altering existing process |     executive       |
| design | process  | flow. The answer should   |     overstepping of |
|        | altered  | demonstrate clearly if    |     powers          |
|        | or       | such change requires any  |                     |
|        | proposed | approval from competent   | b.  Ensures that    |
|        | to be    | authority and if so,      |     necessary       |
|        | altered, | whether the same was      |     legislative and |
|        | to       | obtained or is being      |     administrative  |
|        | acc      | planned.                  |     approvals are   |
|        | ommodate |                           |     taken before    |
|        | AI tool? | **Example answer (for an  |     the AI tool     |
|        |          | AI tool that stops        |     starts altering |
|        | If yes,  | automatic refunds of tax  |     public service  |
|        | what     | in cases of suspicion of  |     delivery        |
|        | a        | tax evasion):** The AI    |     process of the  |
|        | pprovals | tool stops and flags      |     government.     |
|        | are/were | cases of tax evasion and  |                     |
|        | r        | stop the refunds in       |                     |
|        | equired? | suspicious cases till an  |                     |
|        |          | officer verifies the      |                     |
|        | List of  | cause of suspicion and    |                     |
|        | such     | clears the case.          |                     |
|        | a        |                           |                     |
|        | pprovals | There is a significant    |                     |
|        | may be   | process alteration from   |                     |
|        | e        | earlier practice of human |                     |
|        | nclosed. | intervention moving to AI |                     |
|        |          | now. The following        |                     |
|        |          | approvals were taken      |                     |
|        |          | (List).                   |                     |
|        |          |                           |                     |
|        |          | The AI tool is also used  |                     |
|        |          | for launching             |                     |
|        |          | investigative actions and |                     |
|        |          | the additional following  |                     |
|        |          | approvals were taken:     |                     |
|        |          | (List)                    |                     |
+--------+----------+---------------------------+---------------------+

**\
**

**Appendix B -- Algorithm design related questions**

+--------+--------+---------------------------+------------------------+
| Ca     | Quest  | Expected answer           | Purpose                |
| tegory | ion(s) |                           |                        |
+========+========+===========================+========================+
| Source | What   | Answer should include     | a.  Ensures that AI    |
| of     | data   | data source details,      |     tool retains       |
| data   | did    | ownership details, public |     contextual         |
|        | the AI | availability status, and  |     relevance in terms |
|        | train  | details on generation of  |     of data used and   |
|        | on?    | data and completeness.    |     is not built on    |
|        | How    | Any administrative,       |     artificial or      |
|        | was    | legal, or legislative     |     hypothetical data. |
|        | the    | approvals required for    |                        |
|        | data   | using the data should be  | b.  Ensures that there |
|        | gene   | outlined clearly.         |     is clarity on data |
|        | rated, |                           |     gathering process  |
|        | who    | **[Example answer for tax |     and usage of       |
|        | owns   | fraud detection           |     departmental data  |
|        | it?    | AI:]{.underline}**        |     for governance     |
|        | Are    |                           |     purposes. If       |
|        | there  | The tax returns data of   |     private/sensitive  |
|        | any    | past tax delinquents was  |     data is used, the  |
|        | legal  | used along with tax       |     mandate of the     |
|        | or     | returns data of           |     department should  |
|        | ad     | non-delinquents as        |     allow such access  |
|        | minist | examples for training.    |     and use.           |
|        | rative | The department owns the   |                        |
|        | app    | data, the data is         | c.  Ensures necessary  |
|        | rovals | complete and accurate,    |     approvals for      |
|        | needed | and the same is not       |     using data for     |
|        | to use | publicly available as per |     developing AI      |
|        | this   | the business rules (show  |     tool.              |
|        | data?  | why). The tax evaders     |                        |
|        |        | were identified in the    |                        |
|        |        | data by the department    |                        |
|        |        | based on past             |                        |
|        |        | investigation records     |                        |
|        |        | (show how). The following |                        |
|        |        | ddministrative approvals  |                        |
|        |        | were taken for using this |                        |
|        |        | data (List).              |                        |
+--------+--------+---------------------------+------------------------+
| Constr | E      | Should include all        | a.  AI learning from   |
| uction | xplain | features, details or      |     wrong/frivolous    |
| of     | constr | inputs that go into       |     inputs. For        |
| data   | uction | making an example.        |     example, a         |
|        | of     |                           |     person's religion  |
|        | tr     | The training examples     |     should not be      |
|        | aining | shown should include:     |     captured for       |
|        | data   |                           |     developing an AI   |
|        | with   | a\) what was shown as     |     tool to detect tax |
|        | an     | input features            |     evasion. It might  |
|        | ex     |                           |     learn undue biases |
|        | ample. | b\) The labels attached   |     due to this input. |
|        |        | to each example           |                        |
|        |        |                           | b.  Prevents out of    |
|        |        | An example data about a   |     context learning   |
|        |        | person might capture age, |     for AI tool by     |
|        |        | gender, height, race,     |     keeping the        |
|        |        | religion, caste, income,  |     training examples  |
|        |        | education, disability     |     within the scope   |
|        |        | among other inputs. For   |     and range.         |
|        |        | subsidy disbursals,       |                        |
|        |        | certain inputs might be   | c.  AI learning from   |
|        |        | crucial, such as gender   |     insufficient       |
|        |        | or income which should be |     inputs. For        |
|        |        | positively captured in    |     example, AI should |
|        |        | the examples for the AI   |     get adequate       |
|        |        | to learn. The application |     features to learn  |
|        |        | of AI would indicate      |     correctly. An AI   |
|        |        | broad guidelines as to    |     tool that analyzes |
|        |        | what features are         |     anomalies in       |
|        |        | important.                |     subsidy disbursal  |
|        |        |                           |     should have data   |
|        |        |                           |     on gender, age,    |
|        |        |                           |     and demographic    |
|        |        |                           |     features for each  |
|        |        |                           |     example.           |
+--------+--------+---------------------------+------------------------+
| Lab    | Who    | The experts in the        | a.  Prevents biases of |
| elling | la     | subject area should       |     labelers from      |
|        | belled | ideally label the data.   |     becoming an AI     |
|        | the    | The quality of the AI is  |     tool. For example, |
|        | ex     | determined by how good    |     in the agriculture |
|        | amples | the labels are. The       |     produce price      |
|        | for    | biases of the labelers    |     predictor AI, if   |
|        | tra    | are picked up during      |     labelers           |
|        | ining? | training by the AI. This  |     consistently flag  |
|        | E      | answer should clearly     |     small farmers'     |
|        | xplain | explain the process of    |     produce as         |
|        | the    | labeling, and the people  |     inferior, the AI   |
|        | p      | responsible for assigning |     learns to          |
|        | rocess | the labels.               |     discriminate       |
|        | of     |                           |     against small      |
|        | la     | **[Example answer for     |     farmers.           |
|        | beling | food grain price          |                        |
|        | the    | predictor                 | b.  Prevents wrong     |
|        | data.  | AI:]{.underline}**        |     labelling leading  |
|        |        |                           |     to confused AI.    |
|        |        | The data was labelled by  |     The labels may be  |
|        |        | agriculture mandi/market  |     generated by a     |
|        |        | agents. The agents were   |     process which is   |
|        |        | shown the images of       |     flawed. For        |
|        |        | food-grains along with    |     example, if rules  |
|        |        | the quality parameters    |     are applied to     |
|        |        | such as moisture content  |     generate labels    |
|        |        | of grain, harvest         |     about delinquency  |
|        |        | duration etc. Based on    |     of tax filers, the |
|        |        | the price predicted by    |     rules should be    |
|        |        | these human agents, each  |     carefully          |
|        |        | image was labelled.       |     investigated for   |
|        |        | Aggregate value of three  |     correctness,       |
|        |        | unrelated agents from     |     failing which the  |
|        |        | different markets was     |     AI will learn      |
|        |        | taken to avoid collusion  |     these flawed       |
|        |        | or price rigging. The AI  |     rules.             |
|        |        | is expected to learn and  |                        |
|        |        | become at-least as good   |                        |
|        |        | as market agents or       |                        |
|        |        | better. This is measured  |                        |
|        |        | through our evaluation    |                        |
|        |        | methods as listed below.  |                        |
+--------+--------+---------------------------+------------------------+
| Eval   | E      | The evaluation measure    | a.  Avoids machine     |
| uation | xplain | process should match the  |     learning jargon    |
| m      | the    | deployment case. The      |     based evaluation   |
| etrics | depl   | explanation should be     |     measures which may |
|        | oyment | de-jargonized (usage of   |     confuse policy     |
|        | based  | recall, F1 score, ROC     |     makers about true  |
|        | eval   | characteristics curve etc |     efficacy of the AI |
|        | uation | is discouraged) and       |     tool.              |
|        | me     | explained in simple words |                        |
|        | asures | and numbers. A tax fraud  | b.  Customizing        |
|        | of the | detection AI tool should  |     evaluation         |
|        | AI     | not only tell the         |     measures to match  |
|        | tool?  | accuracy (98% accurate),  |     the deployment     |
|        | How    | but also inform how many  |     scenario would     |
|        | good   | wrong cases are flagged   |     help visualize     |
|        | is the | by it (false positive     |     impact on the      |
|        | tool   | rate). The evaluation     |     public once the    |
|        | in     | metrics should be         |     tool is launched.  |
|        | doing  | tailormade for the AI     |                        |
|        | what   | tool. Confusion matrix,   |                        |
|        | it is  | if used/applicable,       |                        |
|        | su     | should be explained in    |                        |
|        | pposed | simple words.             |                        |
|        | to do? |                           |                        |
|        | Does   | **[Example answer for tax |                        |
|        | the    | fraud detection           |                        |
|        | tool   | AI:]{.underline}**        |                        |
|        | commit |                           |                        |
|        | error? | In test 100 test cases    |                        |
|        | If     | given to the AI to        |                        |
|        | yes,   | evaluate the performance, |                        |
|        | what   | there were 20 examples of |                        |
|        | type   | tax avoiders and 980      |                        |
|        | and    | innocent cases.           |                        |
|        | how    |                           |                        |
|        | much?  | Our AI model flagged 30   |                        |
|        |        | cases as tax avoiders. It |                        |
|        |        | identified 18 tax         |                        |
|        |        | avoiders correctly and    |                        |
|        |        | the remaining 12 were     |                        |
|        |        | innocent people flagged   |                        |
|        |        | as tax avoiders by the    |                        |
|        |        | tool. *(The accuracy as   |                        |
|        |        | per the machine learning  |                        |
|        |        | community for the same    |                        |
|        |        | numbers would come to     |                        |
|        |        | 98.8% which is            |                        |
|        |        | misleading)*              |                        |
+--------+--------+---------------------------+------------------------+

**Appendix C -- Fairness, accountability, and ethics related questions**

+------+------------+---------------------------------+---------------+
| Cate | Q          | Expected answer                 | Purpose       |
| gory | uestion(s) |                                 |               |
+======+============+=================================+===============+
| Fair | Do you     | The answer should include the   | a.  AI        |
| ness | suspect    | features used for training and  |     learning  |
| and  | the AI to  | how the data is free from any   |     from      |
| bias | have any   | biases. Special focus should be |     wr        |
|      | biases     | given on biases based on        | ong/frivolous |
|      | based on   | gender, income, age, poverty,   |     inputs.   |
|      | the inputs | religion, caste among others.   |     For       |
|      | supplied   | Some subgroups may not be       |     example,  |
|      | during     | represented well (sparse data   |     a         |
|      | training?  | problem) in the given examples  |     person's  |
|      |            | which needs to be examined in   |     religion  |
|      | What       | the answer.                     |     need not  |
|      | efforts    |                                 |     be        |
|      | have been  | **[Example answer for AI based  |     captured  |
|      | made to    | subsidy determination           |     for an AI |
|      | check      | tool:]{.underline}** The data   |     tool to   |
|      | fairness   | has been tested for balance to  |     detect    |
|      | and to     | ensure that there is no         |     tax       |
|      | ensure     | systematic discrimination among |     evasion.  |
|      | unb        | subgroups based on inputs such  |               |
|      | iasedness? | as gender, religion etc. Each   | b.  AI        |
|      |            | sub-types of examples are       |     learning  |
|      |            | covered adequately for the AI   |     from      |
|      |            | to learn. The results have      |               |
|      |            | again been examined for         |  insufficient |
|      |            | unbiasedness.                   |     inputs.   |
|      |            |                                 |     AI should |
|      |            |                                 |     get       |
|      |            |                                 |     adequate  |
|      |            |                                 |     features  |
|      |            |                                 |     to learn  |
|      |            |                                 |               |
|      |            |                                 | sufficiently. |
|      |            |                                 |     An AI     |
|      |            |                                 |     tool that |
|      |            |                                 |     analyzes  |
|      |            |                                 |     anomalies |
|      |            |                                 |     in        |
|      |            |                                 |     subsidy   |
|      |            |                                 |     disbursal |
|      |            |                                 |     should    |
|      |            |                                 |     have data |
|      |            |                                 |     on        |
|      |            |                                 |     gender,   |
|      |            |                                 |     age, and  |
|      |            |                                 |               |
|      |            |                                 |   demographic |
|      |            |                                 |     features  |
|      |            |                                 |     for each  |
|      |            |                                 |     example   |
|      |            |                                 |     captured  |
|      |            |                                 |     properly. |
+------+------------+---------------------------------+---------------+
| Ac   | Who is     | The department should own any   | a.  Avoids    |
| coun | a          | AI tool that is developed for   |               |
| tabi | ccountable | public use. In case of          |    post-facto |
| lity | for the AI | unintended consequences arising |     a         |
|      | tool going | out of the usage of AI tool,    | ccountability |
|      | wrong?     | the department should clearly   |     fixing.   |
|      |            | identify the roles and          |               |
|      | Is the     | responsibilities to amend/alter | b.  Makes     |
|      | a          | the AI tool for improvements.   |     policy    |
|      | ccountable | The AI tool owner cannot be the |     makers    |
|      | person     | engineering team. The engineers |     cautious  |
|      | named in   | are only responsible for        |     while     |
|      | the answer | development of the tool based   |     green     |
|      | admini     | on the intent of the policy as  |     signaling |
|      | stratively | revealed by the policymakers.   |     AI tool   |
|      | and        | It is the policy makers who are |     for       |
|      | legally    | legally and morally responsible |     public    |
|      | r          | for any AI based governance     |     use.      |
|      | esponsible | tool.                           |               |
|      | for the    |                                 | c.  Forces    |
|      | process?   | This should be displayed in the |     the       |
|      |            | answer.                         |     policy    |
|      |            |                                 |     makers to |
|      |            |                                 |     engage    |
|      |            |                                 |     with the  |
|      |            |                                 |               |
|      |            |                                 |   development |
|      |            |                                 |     process   |
|      |            |                                 |     of the AI |
|      |            |                                 |     tool.     |
+------+------------+---------------------------------+---------------+
| Data | D          | The second question is          | a.  Data      |
| Pri  | emonstrate | important. The person who signs |     privacy   |
| vacy | the data   | off on adequacy of data privacy |     concerns  |
|      | privacy    | measure should be the key       |     are at    |
|      | measures.  | person in the department who is |     the       |
|      |            | responsible for AI tool's       |     forefront |
|      | Who has    | development and maintenance.    |     whenever  |
|      | certified  | This can be the engineering     |     data is   |
|      | them as    | head of the department.         |     collected |
|      | s          |                                 |     by the    |
|      | ufficient? |                                 |               |
|      |            |                                 |   government. |
|      |            |                                 |     The same  |
|      |            |                                 |     should    |
|      |            |                                 |     extend to |
|      |            |                                 |     AI        |
|      |            |                                 |               |
|      |            |                                 |  development. |
+------+------------+---------------------------------+---------------+
| Pu   | What type  | The consultations depend on the | a.  Prevents  |
| blic | of public  | department and the kind of AI   |     security, |
| Cons | or private | tool being developed. For       |     privacy   |
| ulta | con        | public use tools that are not   |     and       |
| tion | sultations | sensitive, wide public          |     ethical   |
|      | were/are   | consultations may be held to    |     concerns  |
|      | held about | address issues of concern. For  |     that may  |
|      | the safety | department based tools and      |     arise and |
|      | of the AI  | investigation tools, security   |     detected  |
|      | system     | and audit agencies within the   |     after the |
|      | before the | government should be engaged to |     launch of |
|      | launch?    | study the tool. Any other       |     tool.     |
|      |            | government department may be    |               |
|      | All        | included under confidence for   | b.  Wider     |
|      | concerns   | such cases.                     |               |
|      | raised by  |                                 | consultations |
|      | the        | The concerns raised by          |     would     |
|      | st         | stakeholders should be          |     generate  |
|      | akeholders | carefully examined for action   |     higher    |
|      | may be     | and demonstrated in the answer. |     trust on  |
|      | listed     |                                 |     the tool  |
|      | along with |                                 |     upon      |
|      | the        |                                 |     launch.   |
|      | remedial   |                                 |               |
|      | measures   |                                 |               |
|      | proposed.  |                                 |               |
+------+------------+---------------------------------+---------------+
| Risk | Show the   | The department should adopt a   | a.  Tools     |
| ev   | risk       | tool/methodology of their       |     such as   |
| alua | evaluation | choice to identify risks        |     FMEA are  |
| tion | carried    | associated with the usage and   |     effective |
|      | out for    | mitigation measures if AI goes  |     and can   |
|      | things     | wrong. Best practices such as   |     ensure    |
|      | that can   | using a Failure Mode Effect     |     that high |
|      | go wrong   | Analysis (FMEA) for AI tool is  |     risk      |
|      | with the   | a good way to go about this.    |               |
|      | AI tool.   | Specific risks about privacy    | possibilities |
|      |            | (data leaks) and adversarial    |     and       |
|      |            | attacks must be covered in the  |     vu        |
|      |            | FMEA or tool of choice.         | lnerabilities |
|      |            |                                 |     in the    |
|      |            | **[Example ideal                |     tool are  |
|      |            | answer:]{.underline}** The risk |     addressed |
|      |            | analysis was carried out to     |     and       |
|      |            | identify severity, occurrence   |               |
|      |            | and detectability of each risk  |    mitigation |
|      |            | that arises from using the AI   |     measures  |
|      |            | tool. The FMEA containing the   |     are put   |
|      |            | risk priority number/scores and |     in place. |
|      |            | mitigation measures is enclosed |               |
|      |            | for reference.                  |               |
+------+------------+---------------------------------+---------------+
| Ex   | Explain    | Depending on the algorithm      | a.  E         |
| plai | how the AI | used, the engineering teams may | xplainability |
| nabi | tool       | or may not be able to come up   |     is a      |
| lity | arrives at | with a good answer. It's        |               |
| of   | the        | difficult to explain a multi    |   cornerstone |
| the  | decisions? | layered neural network based AI |     for       |
| AI   |            | tool in simple language.        |               |
| tool |            | However, simpler AI models can  |   trustworthy |
|      |            | be explained by methods that    |     AI.       |
|      |            | show feature importance. As a   |     Public    |
|      |            | rule of thumb, more trust may   |     would     |
|      |            | be reposed on AI that can be    |     trust an  |
|      |            | explained. Complex algorithms,  |     AI that   |
|      |            | even when they perform better,  |     can be    |
|      |            | might be focusing on features   |     explained |
|      |            | that may induce bias/unfairness |     over a    |
|      |            | and this needs to be verified   |     tool that |
|      |            | thoroughly before allowing the  |     is        |
|      |            | launch.                         |     difficult |
|      |            |                                 |     to        |
|      |            | **Example answer for an AI that |               |
|      |            | determines if a loan may be     |   understand. |
|      |            | given to the business:** (Not   |     The same  |
|      |            | acceptable: The AI uses 4 layer |     goes for  |
|      |            | LSTM Recurrent Neural Network,  |     policy    |
|      |            | with 600 dimensional embeddings |     makers.   |
|      |            | matrix, to arrive at the        |               |
|      |            | decisions. The features that    |               |
|      |            | the AI focuses on do not        |               |
|      |            | contain any bias inducing       |               |
|      |            | elements.) Acceptable: The loan |               |
|      |            | determination is based on a)    |               |
|      |            | past tax filing records, with   |               |
|      |            | suitable weights given to       |               |
|      |            | accurate and timely filing of   |               |
|      |            | taxes. b) The current working   |               |
|      |            | capital requirements to the     |               |
|      |            | assets ratio of the business c) |               |
|      |            | The credit rating of the        |               |
|      |            | firm/business d) Past           |               |
|      |            | delinquency records e) Current  |               |
|      |            | outstanding. Each of them have  |               |
|      |            | the following weights...        |               |
+------+------------+---------------------------------+---------------+
